{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import t3f\n",
    "tf.set_random_seed(0)\n",
    "np.random.seed(0)\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import metric_util as mt\n",
    "import data_util as du\n",
    "from t3f import shapes\n",
    "from nilearn import image\n",
    "import nibabel as nib\n",
    "from math import sqrt\n",
    "import metric_util\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor completion\n",
    "\n",
    "In this example we will see how can we do tensor completion with t3f, i.e. observe a fraction of values in a tensor and recover the rest by assuming that the original tensor has low TT-rank.\n",
    "Mathematically it means that we have a binary mask $P$ and a ground truth tensor $A$, but we observe only a noisy and sparsified version of $A$: $P \\odot (\\hat{A})$, where $\\odot$ is the elementwise product (applying the binary mask) and $\\hat{A} = A + \\text{noise}$. In this case our task reduces to the following optimization problem:\n",
    "\\begin{equation*}\n",
    "\\begin{aligned}\n",
    "& \\underset{X}{\\text{minimize}} \n",
    "& & \\|P \\odot (X - \\hat{A})\\|_F^2 \\\\\n",
    "& \\text{subject to} \n",
    "& & \\text{tt_rank}(X) \\leq r_0\n",
    "\\end{aligned}\n",
    "\\end{equation*}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating problem instance,\n",
    "Lets generate a random matrix $A$, noise, and mask $P$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_scan_path = du.get_full_path_subject1()\n",
    "print \"Subject Path: \" + str(subject_scan_path)\n",
    "x_true_org = mt.read_image_abs_path(subject_scan_path)\n",
    "#x_true_org = image.index_img(x_true_org,1)\n",
    "x_true_org1 = mt.read_image_abs_path(subject_scan_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_true_img = np.array(x_true_org.get_data())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shape = (3, 4, 4, 5, 7, 5)\n",
    "shape = (53,63,46,144)\n",
    "# Fix random seed so the results are comparable between runs.\n",
    "tf.set_random_seed(0)\n",
    "# Generate ground truth tensor A. To make sure that it has low TT-rank,\n",
    "# let's generate a random tt-rank 5 tensor and apply t3f.full to it to convert to actual tensor.\n",
    "#ground_truth = t3f.full(t3f.random_tensor(shape, tt_rank=5))\n",
    "ground_truth = x_true_img\n",
    "# Make a (non trainable) variable out of ground truth. Otherwise, it will be randomly regenerated on each sess.run.\n",
    "ground_truth = tf.get_variable('ground_truth', initializer=ground_truth, trainable=False)\n",
    "noise = 1e-2 * tf.get_variable('noise', initializer=tf.random_normal(shape), trainable=False)\n",
    "noisy_ground_truth = ground_truth + noise\n",
    "# Observe 25% of the tensor values.\n",
    "sparsity_mask = tf.cast(tf.random_uniform(shape) <= 0.80, tf.float32)\n",
    "sparsity_mask = tf.get_variable('sparsity_mask', initializer=sparsity_mask, trainable=False)\n",
    "sparse_observation = noisy_ground_truth * sparsity_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the variable and compute the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frobenius_norm_tf(x):\n",
    "    return tf.reduce_sum(x ** 2) ** 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relative_error1(x_hat,x_true):\n",
    "    percent_error = frobenius_norm_tf(x_hat - x_true) / (frobenius_norm_tf(x_true))\n",
    "    return percent_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observed_total = tf.reduce_sum(sparsity_mask)\n",
    "total = np.prod(shape)\n",
    "ranks_a = np.array([53,63,46,144,1])\n",
    "tt_with_ranks = t3f.to_tt_tensor(x_true_img, max_tt_rank=144)\n",
    "ranks = shapes.tt_ranks(tt_with_ranks)\n",
    "initialization = t3f.random_tensor(shape, tt_rank=10)\n",
    "estimated = t3f.get_variable('estimated', initializer=initialization)\n",
    "# Loss is MSE between the estimated and ground-truth tensor as computed in the observed cells.\n",
    "loss = tf.reduce_sum((sparsity_mask * t3f.full(estimated) - sparse_observation)**2)/(tf.reduce_sum(sparse_observation)**2)\n",
    "# Test loss is MSE between the estimated tensor and full (and not noisy) ground-truth tensor A.\n",
    "test_loss = tf.reduce_sum((t3f.full(estimated) - ground_truth)**2)/(tf.reduce_sum(ground_truth)**2)\n",
    "rel_error1 = relative_error1(t3f.full(estimated), ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SGD optimization\n",
    "The simplest way to solve the optimization problem is Stochastic Gradient Descent: let TensorFlow differentiate the loss w.r.t. the factors (cores) of the TensorTrain decomposition of the estimated tensor and minimize the loss with your favourite SGD variation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.01, epsilon=1e-18)\n",
    "step = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "train_loss_hist = []\n",
    "test_loss_hist = []\n",
    "for i in range(2):\n",
    "    _, tr_loss_v, test_loss_v, rel_error1_v, ranks_v = sess.run([step, loss, test_loss,rel_error1, ranks])\n",
    "    train_loss_hist.append(tr_loss_v)\n",
    "    test_loss_hist.append(test_loss_v)\n",
    "    print(i, tr_loss_v, test_loss_v, rel_error1_v, ranks_v)\n",
    "    #if i % 1000 == 0:\n",
    "     #   print(i, tr_loss_v, test_loss_v, rel_error1_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.loglog(train_loss_hist, label='train')\n",
    "plt.loglog(test_loss_hist, label='test')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('MSE Loss value')\n",
    "plt.title('SGD completion')\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ground_truth_var = t3f.get_variable('ground_truth', initializer=ground_truth, reuse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ground_truth.read_value()\n",
    "ground_truth_val = ground_truth.eval(session=sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimated_val = sess.run(t3f.full(estimated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relative_error(x_hat,x_true):\n",
    "    percent_error = np.linalg.norm(x_hat - x_true) / np.linalg.norm(x_true)\n",
    "    return percent_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_error = relative_error(estimated_val,ground_truth_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ten_ones = np.ones_like(mask)\n",
    "#x_reconstr = mt.reconstruct(x_hat,x_true, ten_ones, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimated_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn import image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = (53,63,46,144)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_observation_val=sparse_observation.eval(session=sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_miss_img = mt.reconstruct_image_affine(x_true_org, sparse_observation_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_miss = x_miss_img\n",
    "x_miss = image.index_img(x_miss_img,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_hat_img = mt.reconstruct_image_affine(x_true_org, estimated_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_hat = x_hat_img\n",
    "x_hat = image.index_img(x_hat_img,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn import plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_true_org = image.index_img(x_true_org,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recovered_image = plotting.plot_epi(x_hat, bg_img=None,black_bg=True, cmap='jet', cut_coords=None) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_miss_image = plotting.plot_epi(x_miss, bg_img=None,black_bg=True, cmap='jet', cut_coords=None) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ellipsoid_masker as elpm\n",
    "import ellipsoid_mask as em"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_corrupted_image(x0,y0,z0, x_r, y_r):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_structural_missing_pattern(x0,y0,z0, x_r, y_r, frames_count, path_folder):\n",
    "    subject_scan_path = du.get_full_path_subject1()\n",
    "        \n",
    "    print (\"3D Random Missing Value Pattern Simulations has started...\")\n",
    "    print \"Subject Path: \" + str(subject_scan_path)\n",
    "    \n",
    "    n = 0\n",
    "    # type 1 (center is the center of the image), corrupt first 10 frames\n",
    "    #x0, y0, z0 = (0 ,-18 , 17)\n",
    "    #x_r, y_r, z_r = (20, 17, 15)\n",
    "    \n",
    "    print \"===Type 1 Experiments====\"\n",
    "    \n",
    "    target_img = image.index_img(subject_scan_path,n)\n",
    "    \n",
    "    type_1_folder_path = path_folder\n",
    "    masked_img_file_path  = type_1_folder_path + \"/\" + \"size_\" + str(x_r) + \"_\" + str(y_r) + \"_\" + str(z_r) + \"_scan_\" + str(n)\n",
    "    \n",
    "    corrupted_volumes_list = []\n",
    "    corrupted_volumes_list_scan_numbers = []\n",
    "    \n",
    "    for i in xrange(frames_count):\n",
    "        masked_img_file_path  = type_1_folder_path + \"/\" + \"size_\" + str(x_r) + \"_\" + str(y_r) + \"_\" + str(z_r) + \"_scan_\" + str(i)\n",
    "        target_img = image.index_img(subject_scan_path,i)\n",
    "        image_masked_by_ellipsoid = elpm.create_ellipsoid_mask(x0, y0, z0, x_r, y_r, z_r, target_img, masked_img_file_path)\n",
    "        \n",
    "        masked_img_file_path = masked_img_file_path + \".nii\"\n",
    "        ellipsoid = em.EllipsoidMask(x0, y0, z0, x_r, y_r, z_r, masked_img_file_path)\n",
    "        ellipsoid_volume = ellipsoid.volume()\n",
    "        observed_ratio = mt.compute_observed_ratio(image_masked_by_ellipsoid)\n",
    "        \n",
    "        corrupted_volumes_list.append(image_masked_by_ellipsoid)\n",
    "        corrupted_volumes_list_scan_numbers.append(i)\n",
    "        print (\"Ellipsoid Volume: \" + str(ellipsoid_volume) + \"; Missing Ratio: \" + str(observed_ratio))\n",
    "    \n",
    "    # now create corrupted 4d where fist 10 frames has ellipsoid missing across 10 frames\n",
    "    counter = 0\n",
    "    \n",
    "    volumes_list = []\n",
    "    for img in image.iter_img(subject_scan_path):\n",
    "        print \"Volume Index: \" + str(counter)\n",
    "        if counter in corrupted_volumes_list_scan_numbers:\n",
    "            print \"Adding corrupted volume to the list \" + str(counter)\n",
    "            volumes_list.append(corrupted_volumes_list[counter])\n",
    "        else:\n",
    "            print \"Adding normal volume to the list \" + str(counter)\n",
    "            volumes_list.append(img)\n",
    "        counter = counter + 1\n",
    "        \n",
    "    # now generate corrupted 4D from the list\n",
    "    corrupted4d_10 = image.concat_imgs(volumes_list)\n",
    "    print \"Corrupted 4D - 10 frames: \" + str(corrupted4d_10)\n",
    "    observed_ratio4D_10 = mt.compute_observed_ratio(corrupted4d_10)\n",
    "    print (\"Corrupted 4D - 10 Volume: \" + \"; Missing Ratio: \" + str(observed_ratio4D_10))\n",
    "    corr_file_path4D = du.corrupted4D_10_frames_path()\n",
    "    nib.save(corrupted4d_10, corr_file_path4D)\n",
    "    return corrupted4d_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_xyz(i, j, k, epi_img):\n",
    "    M = epi_img.affine[:3, :3]\n",
    "    abc = epi_img.affine[:3, 3]\n",
    "    return M.dot([i, j, k]) + abc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coord = [26,31,23]\n",
    "path_folder3D = \"/work/el/3D\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_true_org3D = image.index_img(subject_scan_path,0)\n",
    "nib.save(x_true_org3D,path_folder3D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_coord  = get_xyz(10,7,4, x_true_org3D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_coord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_coord_center  = get_xyz(26,31,23, x_true_org3D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_coord_center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_folder3D = \"/work/el/3D\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_folder = \"/work/el/75\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x0, y0, z0 = (-10, -20,17)\n",
    "#x0, y0, z0 = (-5, -20,17)\n",
    "#x0, y0, z0 = (2, 32,22)\n",
    "#x_r, y_r, z_r = (20,17,15)\n",
    "x0, y0, z0 = (2, 32,22)\n",
    "# size 1\n",
    "#x_r, y_r, z_r = (7,10,8)\n",
    "# size 2\n",
    "#x_r, y_r, z_r = (9,10,8)\n",
    "#size 3\n",
    "x_r, y_r, z_r = (12,10,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_miss_img = generate_structural_missing_pattern(x0,y0,z0, x_r, y_r, 1, path_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_miss_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_miss_img_data = np.array(x_miss_img.get_data())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = elpm.ellipsoid_masker(x_r, y_r, z_r, x0, y0, z0, x_true_org3D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_data = np.array(mask.get_data())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_system_noise_roi_mask(img, snr_db, mask):\n",
    "    snr = sqrt(np.power(10.0, snr_db / 10.0))\n",
    "    print (\"snr: \" + str(snr))\n",
    "    data = np.array(img.get_data())\n",
    "    signal = data[mask > 0].reshape(-1)\n",
    "    sigma_n = signal.mean() / snr\n",
    "    print (\"sigma_n: \" + str(sigma_n))\n",
    "    n_1 = np.random.normal(size=data.shape, scale=sigma_n)\n",
    "    n_2 = np.random.normal(size=data.shape, scale=sigma_n)\n",
    "    stde_1 = n_1 / sqrt(2.0)\n",
    "    stde_2 = n_2 / sqrt(2.0)\n",
    "    im_noise = np.sqrt((data + stde_1)**2 + (stde_2)**2)\n",
    "    im_noise[mask == 0] = 0\n",
    "    noise_idxs = np.where(im_noise > 0)\n",
    "    data[noise_idxs] = im_noise[noise_idxs]\n",
    "    return data, im_noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_noisy_image(x, snr_db, mask):\n",
    "    x_noisy, noise_mask = generate_system_noise_roi_mask(x, snr_db, mask)\n",
    "    x_noisy_img = mt.reconstruct_image_affine(x, x_noisy)\n",
    "    noise_mask_img = mt.reconstruct_image_affine(x, noise_mask)\n",
    "    return x_noisy_img, noise_mask_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noisy_roi = generate_system_noise(x_true_org3D, 2, mask_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x = np.array(x_true_org3D.get_data())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = data_x*(1./np.linalg.norm(data_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1./np.linalg.norm(self.x_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs = np.where(noisy_roi > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1[mask_data == 1] = 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = data1 + noisy_roi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1[x_r +2 , y_r +2, z_r +2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noisy_roi_img = mt.reconstruct_image_affine(x_true_org3D, noisy_roi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_n, noise_mask = create_noisy_image(x_true_org3D, 2, mask_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss[x_r +2 , y_r +2, z_r +2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disp = plotting.plot_img(x_true_org3D, bg_img=None,black_bg=True, cmap='jet', cut_coords=[x0, y0, z0]) \n",
    "#disp.add_contours(mask, levels=[0.1, 0.3, 0.4, 0.5], filled=False, colors='b')\n",
    "disp.add_overlay(noise_mask, alpha = 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "volumes_list = []\n",
    "for img in image.iter_img(subject_scan_path):\n",
    "    print \"Volume Index: \" + str(counter)\n",
    "    if counter == 0:\n",
    "        print \"Adding corrupted volume to the list \" + str(counter)\n",
    "        volumes_list.append(x_n)\n",
    "    else:\n",
    "        print \"Adding normal volume to the list \" + str(counter)\n",
    "        volumes_list.append(img)\n",
    "        counter = counter + 1\n",
    "        \n",
    "    # now generate corrupted 4D from the list\n",
    "x_corr_img = image.concat_imgs(volumes_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_img_data = np.array(x_corr_img.get_data())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print mask_img_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print mask_indices_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_indices = np.ones_like(mask_img_data)\n",
    "mask_indices[mask_img_data == 0] = 0.0\n",
    "mask_indices_img = mt.reconstruct_image_affine(x_true_org1, mask_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.masking import apply_mask\n",
    "masked_data = apply_mask(x_corr_img, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print x_corr_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.plot(masked_data[:148, :25])\n",
    "plt.xlabel('Time [TRs]', fontsize=16)\n",
    "plt.ylabel('Intensity', fontsize=16)\n",
    "plt.xlim(0, 143)\n",
    "plt.subplots_adjust(bottom=.12, top=.95, right=.95, left=.12)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_miss_img_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print x_true_org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_img = image.index_img(x_true_org1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_img_file_path  = \"/work/el\" + \"/\" + \"size_\" + str(x_r) + \"_\" + str(y_r) + \"_\" + str(z_r) + \"_scan_\" + str(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_img_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_masked_by_ellipsoid = elpm.create_ellipsoid_mask(x0, y0, z0, x_r, y_r, z_r, target_img, masked_img_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_masked_by_ellipsoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spectrum_mask(size):\n",
    "    \"\"\"Creates a mask to filter the image of size size\"\"\"\n",
    "    import numpy as np\n",
    "    from scipy.ndimage.morphology import distance_transform_edt as distance\n",
    "\n",
    "    ftmask = np.ones(size)\n",
    "\n",
    "    # Set zeros on corners\n",
    "    # ftmask[0, 0] = 0\n",
    "    # ftmask[size[0] - 1, size[1] - 1] = 0\n",
    "    # ftmask[0, size[1] - 1] = 0\n",
    "    # ftmask[size[0] - 1, 0] = 0\n",
    "    ftmask[size[0] // 2, size[1] // 2] = 0\n",
    "\n",
    "    # Distance transform\n",
    "    ftmask = distance(ftmask)\n",
    "    ftmask /= ftmask.max()\n",
    "\n",
    "    # Keep this just in case we want to switch to the opposite filter\n",
    "    ftmask *= -1.0\n",
    "    ftmask += 1.0\n",
    "\n",
    "    ftmask[ftmask >= 0.4] = 1\n",
    "    ftmask[ftmask < 1] = 0\n",
    "    return ftmask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slice_wise_fft(in_file, ftmask=None, spike_thres=3., out_prefix=None):\n",
    "    \"\"\"Search for spikes in slices using the 2D FFT\"\"\"\n",
    "    import os.path as op\n",
    "    import numpy as np\n",
    "    import nibabel as nb\n",
    "    from scipy.ndimage.filters import median_filter\n",
    "    from scipy.ndimage import generate_binary_structure, binary_erosion\n",
    "    from statsmodels.robust.scale import mad\n",
    "\n",
    "    if out_prefix is None:\n",
    "        fname, ext = op.splitext(op.basename(in_file))\n",
    "        if ext == '.gz':\n",
    "            fname, _ = op.splitext(fname)\n",
    "        out_prefix = op.abspath(fname)\n",
    "\n",
    "    func_data = nb.load(in_file).get_data()\n",
    "\n",
    "    if ftmask is None:\n",
    "        ftmask = spectrum_mask(tuple(func_data.shape[:2]))\n",
    "\n",
    "    fft_data = []\n",
    "    for t in range(func_data.shape[-1]):\n",
    "        func_frame = func_data[..., t]\n",
    "        fft_slices = []\n",
    "        for z in range(func_frame.shape[2]):\n",
    "            sl = func_frame[..., z]\n",
    "            fftsl = median_filter(np.real(np.fft.fft2(sl)).astype(np.float32),\n",
    "                                  size=(5, 5), mode='constant') * ftmask\n",
    "            fft_slices.append(fftsl)\n",
    "        fft_data.append(np.stack(fft_slices, axis=-1))\n",
    "\n",
    "    # Recompose the 4D FFT timeseries\n",
    "    fft_data = np.stack(fft_data, -1)\n",
    "\n",
    "    # Z-score across t, using robust statistics\n",
    "    mu = np.median(fft_data, axis=3)\n",
    "    sigma = np.stack([mad(fft_data, axis=3)] * fft_data.shape[-1], -1)\n",
    "    idxs = np.where(np.abs(sigma) > 1e-4)\n",
    "    fft_zscored = fft_data - mu[..., np.newaxis]\n",
    "    fft_zscored[idxs] /= sigma[idxs]\n",
    "\n",
    "    # save fft z-scored\n",
    "    out_fft = op.abspath(out_prefix + '_zsfft.nii.gz')\n",
    "    nii = nb.Nifti1Image(fft_zscored.astype(np.float32), np.eye(4), None)\n",
    "    nii.to_filename(out_fft)\n",
    "\n",
    "    # Find peaks\n",
    "    spikes_list = []\n",
    "    for t in range(fft_zscored.shape[-1]):\n",
    "        fft_frame = fft_zscored[..., t]\n",
    "\n",
    "        for z in range(fft_frame.shape[-1]):\n",
    "            sl = fft_frame[..., z]\n",
    "            if np.all(sl < spike_thres):\n",
    "                continue\n",
    "\n",
    "            # Any zscore over spike_thres will be called a spike\n",
    "            sl[sl <= spike_thres] = 0\n",
    "            sl[sl > 0] = 1\n",
    "\n",
    "            # Erode peaks and see how many survive\n",
    "            struc = generate_binary_structure(2, 2)\n",
    "            sl = binary_erosion(sl.astype(np.uint8), structure=struc).astype(np.uint8)\n",
    "\n",
    "            if sl.sum() > 10:\n",
    "                print ((t, z), sl.sum() )\n",
    "                spikes_list.append((t, z))\n",
    "\n",
    "    out_spikes = op.abspath(out_prefix + '_spikes.tsv')\n",
    "    np.savetxt(out_spikes, spikes_list, fmt=b'%d', delimiter=b'\\t', header='TR\\tZ')\n",
    "\n",
    "    return len(spikes_list), out_spikes, out_fft, spikes_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.masking import compute_epi_mask\n",
    "mask_img1 = np.array(compute_epi_mask(subject_scan_path).get_data())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_spikes, out_spikes, out_fft, spikes_list = slice_wise_fft(subject_scan_path, spike_thres=4.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_fft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_spikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spiked_fft = mt.read_image_abs_path(out_fft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print spiked_fft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_6_img = image.index_img(x_true_org1, 39)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_fft_img = image.mean_img(spiked_fft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spike_6_img = image.index_img(spiked_fft,39)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spike_6 = plotting.plot_img(spike_6_img, display_mode='z', bg_img=None,black_bg=True, cmap='Greys_r', cut_coords=[18])\n",
    "spike_6.add_contours(z_score_d_mask_img, levels=[0.5], filled=True, alpha=0.8, colors='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_6 = plotting.plot_epi(tr_6_img, display_mode='z', bg_img=None,black_bg=True,cut_coords=[18]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#z_score_epi_mask = get_z_score_robust_spatial_mask(tr_6_img, 4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_z_score_robust_spatial_mask(x_img, z_score_cut_off):\n",
    "    mu = np.median(np.array(x_img.get_data()))\n",
    "    sigma = np.stack([mad(np.array(x_img.get_data()))] * np.array(x_img.get_data()).shape[-1], -1)\n",
    "    idxs = np.where(np.abs(sigma) > 1e-10)\n",
    "    ground_truth_z_score = np.array(x_img.get_data()) - mu[..., np.newaxis]\n",
    "    ground_truth_z_score[idxs] /= sigma[idxs]\n",
    "    mask_z_score_indices = (abs(ground_truth_z_score) > z_score_cut_off).astype('int')\n",
    "    print (\"Z-score indices count: \" + str(get_mask_z_indices_count(mask_z_score_indices)))\n",
    "    return mask_z_score_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_z_score_robust_mask(x_img, z_score_cut_off):\n",
    "    mu = np.median(np.array(x_img.get_data()), axis=3)\n",
    "    sigma = np.stack([mad(np.array(x_img.get_data()), axis=3)] * np.array(x_img.get_data()).shape[-1], -1)\n",
    "    idxs = np.where(np.abs(sigma) > 1e-10)\n",
    "    ground_truth_z_score = np.array(x_img.get_data()) - mu[..., np.newaxis]\n",
    "    ground_truth_z_score[idxs] /= sigma[idxs]\n",
    "    mask_z_score_indices = (abs(ground_truth_z_score) > z_score_cut_off).astype('int')\n",
    "    print (\"Z-score indices count: \" + str(get_mask_z_indices_count(mask_z_score_indices)))\n",
    "    return mask_z_score_indices\n",
    "\n",
    "def get_mask_z_indices_count(mask_z_score):\n",
    "    mask_z_indices_count = np.count_nonzero(mask_z_score==1)\n",
    "    return mask_z_indices_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.robust.scale import mad\n",
    "z_score_d_mask = get_z_score_robust_spatial_mask(spike_6_img, 4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_score_mask = get_z_score_robust_mask(x_true_org1, 4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_score_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_score_d_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_score_d_mask_img = image.new_img_like(spike_6_img,z_score_d_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spike_6_with_overlay = plotting.plot_img(spike_6_img, display_mode='z', bg_img=None,black_bg=True, cmap='gray', colorbar=True, cut_coords=10)\n",
    "spike_6_with_overlay.add_contours(z_score_d_mask_img, levels=[0.5], filled=True, alpha=0.8, colors='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_6 = plotting.plot_epi(tr_6_img, display_mode='z', bg_img=None,black_bg=True, cut_coords=10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_score_fft = get_z_score_robust_mask(spiked_fft, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_score_fft_img = image.new_img_like(spiked_fft,z_score_fft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_data = apply_mask(spiked_fft, z_score_d_mask_img)\n",
    "\n",
    "# masked_data shape is (timepoints, voxels). We can plot the first 150\n",
    "# timepoints from two voxels\n",
    "\n",
    "# And now plot a few of these\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.plot(masked_data[:144, :3])\n",
    "plt.xlabel('Time [TRs]', fontsize=16)\n",
    "plt.ylabel('Intensity', fontsize=16)\n",
    "plt.xlim(0, 150)\n",
    "plt.subplots_adjust(bottom=.12, top=.95, right=.95, left=.12)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_data_t = apply_mask(spiked_fft, z_score_d_mask_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_spikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_spikes(in_file, in_fft, spikes_list, cols=3,\n",
    "                labelfmt='t={0:.3f}s (z={1:d})',\n",
    "                out_file=None):\n",
    "    from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "    nii = nb.as_closest_canonical(nb.load(in_file))\n",
    "    fft = nb.load(in_fft).get_data()\n",
    "\n",
    "    data = nii.get_data()\n",
    "    zooms = nii.header.get_zooms()[:2]\n",
    "    tstep = nii.header.get_zooms()[-1]\n",
    "    ntpoints = data.shape[-1]\n",
    "\n",
    "    if len(spikes_list) > cols * 7:\n",
    "        cols += 1\n",
    "\n",
    "    nspikes = len(spikes_list)\n",
    "    rows = 1\n",
    "    if nspikes > cols:\n",
    "        rows = math.ceil(nspikes / cols)\n",
    "\n",
    "    fig = plt.figure(figsize=(7 * cols, 5 * rows))\n",
    "\n",
    "    for i, (t, z) in enumerate(spikes_list):\n",
    "        prev = None\n",
    "        pvft = None\n",
    "        if t > 0:\n",
    "            prev = data[..., z, t - 1]\n",
    "            pvft = fft[..., z, t - 1]\n",
    "\n",
    "        post = None\n",
    "        psft = None\n",
    "        if t < (ntpoints - 1):\n",
    "            post = data[..., z, t + 1]\n",
    "            psft = fft[..., z, t + 1]\n",
    "\n",
    "        ax1 = fig.add_subplot(rows, cols, i + 1)\n",
    "        divider = make_axes_locatable(ax1)\n",
    "        ax2 = divider.new_vertical(size=\"100%\", pad=0.1)\n",
    "        fig.add_axes(ax2)\n",
    "\n",
    "        plot_slice_tern(data[..., z, t], prev=prev, post=post, spacing=zooms,\n",
    "                        ax=ax2,\n",
    "                        label=labelfmt.format(t * tstep, z))\n",
    "\n",
    "        plot_slice_tern(fft[..., z, t], prev=pvft, post=psft, vmin=-5, vmax=5,\n",
    "                        cmap='binary', ax=ax1)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    if out_file is None:\n",
    "        fname, ext = op.splitext(op.basename(in_file))\n",
    "        if ext == '.gz':\n",
    "            fname, _ = op.splitext(fname)\n",
    "        out_file = op.abspath('%s.svg' % fname)\n",
    "\n",
    "    fig.savefig(out_file, format='svg', dpi=300, bbox_inches='tight')\n",
    "    return out_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os.path as op\n",
    "import numpy as np\n",
    "import nibabel as nb\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from matplotlib.backends.backend_pdf import FigureCanvasPdf as FigureCanvas\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_slice_tern(dslice, prev=None, post=None,\n",
    "                    spacing=None, cmap='Greys_r', label=None, ax=None,\n",
    "                    vmax=None, vmin=None):\n",
    "    from matplotlib.cm import get_cmap\n",
    "\n",
    "    if isinstance(cmap, (str, bytes)):\n",
    "        cmap = get_cmap(cmap)\n",
    "\n",
    "    est_vmin, est_vmax = _get_limits(dslice)\n",
    "    if not vmin:\n",
    "        vmin = est_vmin\n",
    "    if not vmax:\n",
    "        vmax = est_vmax\n",
    "\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "\n",
    "    if spacing is None:\n",
    "        spacing = [1.0, 1.0]\n",
    "    else:\n",
    "        spacing = [spacing[1], spacing[0]]\n",
    "\n",
    "    phys_sp = np.array(spacing) * dslice.shape\n",
    "\n",
    "    if prev is None:\n",
    "        prev = np.ones_like(dslice)\n",
    "    if post is None:\n",
    "        post = np.ones_like(dslice)\n",
    "\n",
    "    combined = np.swapaxes(np.vstack((prev, dslice, post)), 0, 1)\n",
    "    ax.imshow(combined, vmin=vmin, vmax=vmax, cmap=cmap,\n",
    "              interpolation='nearest', origin='lower',\n",
    "              extent=[0, phys_sp[1] * 3, 0, phys_sp[0]])\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "    ax.grid(False)\n",
    "\n",
    "    if label is not None:\n",
    "        ax.text(.5, .05, label,\n",
    "                transform=ax.transAxes,\n",
    "                horizontalalignment='center',\n",
    "                verticalalignment='top',\n",
    "                size=14,\n",
    "                bbox=dict(boxstyle=\"square,pad=0\", ec='k', fc='k'),\n",
    "                color='w')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parula():\n",
    "    from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "    cm_data = [\n",
    "        [0.2081, 0.1663, 0.5292],\n",
    "        [0.2116238095, 0.1897809524, 0.5776761905],\n",
    "        [0.212252381, 0.2137714286, 0.6269714286],\n",
    "        [0.2081, 0.2386, 0.6770857143],\n",
    "        [0.1959047619, 0.2644571429, 0.7279],\n",
    "        [0.1707285714, 0.2919380952, 0.779247619],\n",
    "        [0.1252714286, 0.3242428571, 0.8302714286],\n",
    "        [0.0591333333, 0.3598333333, 0.8683333333],\n",
    "        [0.0116952381, 0.3875095238, 0.8819571429],\n",
    "        [0.0059571429, 0.4086142857, 0.8828428571],\n",
    "        [0.0165142857, 0.4266, 0.8786333333],\n",
    "        [0.032852381, 0.4430428571, 0.8719571429],\n",
    "        [0.0498142857, 0.4585714286, 0.8640571429],\n",
    "        [0.0629333333, 0.4736904762, 0.8554380952],\n",
    "        [0.0722666667, 0.4886666667, 0.8467],\n",
    "        [0.0779428571, 0.5039857143, 0.8383714286],\n",
    "        [0.079347619, 0.5200238095, 0.8311809524],\n",
    "        [0.0749428571, 0.5375428571, 0.8262714286],\n",
    "        [0.0640571429, 0.5569857143, 0.8239571429],\n",
    "        [0.0487714286, 0.5772238095, 0.8228285714],\n",
    "        [0.0343428571, 0.5965809524, 0.819852381],\n",
    "        [0.0265, 0.6137, 0.8135],\n",
    "        [0.0238904762, 0.6286619048, 0.8037619048],\n",
    "        [0.0230904762, 0.6417857143, 0.7912666667],\n",
    "        [0.0227714286, 0.6534857143, 0.7767571429],\n",
    "        [0.0266619048, 0.6641952381, 0.7607190476],\n",
    "        [0.0383714286, 0.6742714286, 0.743552381],\n",
    "        [0.0589714286, 0.6837571429, 0.7253857143],\n",
    "        [0.0843, 0.6928333333, 0.7061666667],\n",
    "        [0.1132952381, 0.7015, 0.6858571429],\n",
    "        [0.1452714286, 0.7097571429, 0.6646285714],\n",
    "        [0.1801333333, 0.7176571429, 0.6424333333],\n",
    "        [0.2178285714, 0.7250428571, 0.6192619048],\n",
    "        [0.2586428571, 0.7317142857, 0.5954285714],\n",
    "        [0.3021714286, 0.7376047619, 0.5711857143],\n",
    "        [0.3481666667, 0.7424333333, 0.5472666667],\n",
    "        [0.3952571429, 0.7459, 0.5244428571],\n",
    "        [0.4420095238, 0.7480809524, 0.5033142857],\n",
    "        [0.4871238095, 0.7490619048, 0.4839761905],\n",
    "        [0.5300285714, 0.7491142857, 0.4661142857],\n",
    "        [0.5708571429, 0.7485190476, 0.4493904762],\n",
    "        [0.609852381, 0.7473142857, 0.4336857143],\n",
    "        [0.6473, 0.7456, 0.4188],\n",
    "        [0.6834190476, 0.7434761905, 0.4044333333],\n",
    "        [0.7184095238, 0.7411333333, 0.3904761905],\n",
    "        [0.7524857143, 0.7384, 0.3768142857],\n",
    "        [0.7858428571, 0.7355666667, 0.3632714286],\n",
    "        [0.8185047619, 0.7327333333, 0.3497904762],\n",
    "        [0.8506571429, 0.7299, 0.3360285714],\n",
    "        [0.8824333333, 0.7274333333, 0.3217],\n",
    "        [0.9139333333, 0.7257857143, 0.3062761905],\n",
    "        [0.9449571429, 0.7261142857, 0.2886428571],\n",
    "        [0.9738952381, 0.7313952381, 0.266647619],\n",
    "        [0.9937714286, 0.7454571429, 0.240347619],\n",
    "        [0.9990428571, 0.7653142857, 0.2164142857],\n",
    "        [0.9955333333, 0.7860571429, 0.196652381],\n",
    "        [0.988, 0.8066, 0.1793666667],\n",
    "        [0.9788571429, 0.8271428571, 0.1633142857],\n",
    "        [0.9697, 0.8481380952, 0.147452381],\n",
    "        [0.9625857143, 0.8705142857, 0.1309],\n",
    "        [0.9588714286, 0.8949, 0.1132428571],\n",
    "        [0.9598238095, 0.9218333333, 0.0948380952],\n",
    "        [0.9661, 0.9514428571, 0.0755333333],\n",
    "        [0.9763, 0.9831, 0.0538]]\n",
    "\n",
    "    return LinearSegmentedColormap.from_list('parula', cm_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_limits(nifti_file, only_plot_noise=False):\n",
    "    if isinstance(nifti_file, str):\n",
    "        nii = nb.as_closest_canonical(nb.load(nifti_file))\n",
    "        data = nii.get_data()\n",
    "    else:\n",
    "        data = nifti_file\n",
    "\n",
    "    data_mask = np.logical_not(np.isnan(data))\n",
    "\n",
    "    if only_plot_noise:\n",
    "        data_mask = np.logical_and(data_mask, data != 0)\n",
    "        vmin = np.percentile(data[data_mask], 0)\n",
    "        vmax = np.percentile(data[data_mask], 61)\n",
    "    else:\n",
    "        vmin = np.percentile(data[data_mask], 0.5)\n",
    "        vmax = np.percentile(data[data_mask], 99.5)\n",
    "\n",
    "    return vmin, vmax\n",
    "\n",
    "\n",
    "def _bbox(img_data, bbox_data):\n",
    "    B = np.argwhere(bbox_data)\n",
    "    (ystart, xstart, zstart), (ystop, xstop, zstop) = B.min(0), B.max(0) + 1\n",
    "    return img_data[ystart:ystop, xstart:xstop, zstart:zstop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_spikes(subject_scan_path, out_fft, spikes_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_spikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "        print i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
